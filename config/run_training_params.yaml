# LLaMA-2-7B causal LM experiments with ARD-LoRA and Bayesian-PEFT datasets
defaults:
  runId: 1
  # Model configuration
  model_name: "LLaMA2-7B"
  model_name_or_path: "meta-llama/Llama-2-7b-hf"
  tokenizer_name: null  # Use model tokenizer
  train_type: "causal_lm"  # Specify training type for validation
  dataset_name: "BayesianPEFT"
  dataset_name_specific: "sst2"  # Which specific dataset from Bayesian-PEFT
  
  # Cache configuration
  cache_root: "/content/drive/MyDrive/ARD_LoRA_Data_Cache"  # Google Drive cache
  google_drive_cache: "/content/drive/MyDrive/ARD_LoRA_Data_Cache"  # Google Drive path
  use_google_drive: true  # Enable Google Drive caching
  
  # Training hyperparameters (adapted from Section 4.1 of BLoB / best-effort)
  learning_rate: 0.0001  # 1e-4 as float
  lr_scheduler_type: "linear"
  warmup_ratio: 0.03
  weight_decay: 0.0
  train_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 16
  
  # Precision Configuration
  fp16: false
  bf16: true  # Preferred on A100 GPUs for better performance
  load_in_4bit: false  # Disable quantization - LoRA parameters need to be trainable
  
  # Training Strategy
  save_strategy: "epoch"
  eval_strategy: "epoch"
  save_steps: 500
  eval_steps: null
  load_best_model_at_end: true
  remove_unused_columns: false
  
  # Optimization
  max_grad_norm: 1.0
  optim: "adamw_torch"
  gradient_checkpointing: true
  use_cache: false  # Disable KV caching during training to save memory
  
  # Logging and Monitoring
  logging_steps: 50
  dataloader_num_workers: 0
  
  # ARD-LoRA Configuration
  rank: 16
  scaling: 1.0
  max_lora_rank_threshold: 64
  
  # ProbLoRA Model Parameters
  num_tokens: 2048
  ard_prior_samples: 100
  
  # Numerical Stability Parameters
  logvar_clamp_min: -15.0
  logvar_clamp_max: 15.0
  beta_logvar_clamp_min: -10.0
  beta_logvar_clamp_max: 10.0
  sample_clamp_min: -50.0
  sample_clamp_max: 50.0
  
  # ARD Loss Configuration
  kl_loss_beta: 0.01  # KL divergence loss strength for ARD-LoRA
  
  # ARD Relevance Thresholds
  ard_high_relevance_threshold: 0.1
  ard_medium_relevance_threshold: 0.01
  
  # Uncertainty Evaluation
  uncertainty_eval_samples: 1000
  uncertainty_n_bins: 15
  
  # Dataset Configuration
  max_len: 2048
  num_labels: 0
  validation_split_ratio: 0.1
  min_validation_samples: 2000
  max_ard_ratio: 0.8
  random_seed: 42
  max_validation_samples: 250  # Cap validation dataset size for memory efficiency
  
  # Callback Configuration
  enable_callbacks: true
  enable_plotting: true
  enable_resampling: true
  plot_start_epoch: 2
  plot_interval: 2
  plot_batch_size: 16
  
  # Memory Optimization
  gpu_memory_fraction: 0.9
  gradient_checkpointing_kwargs:
    use_reentrant: false
  
  # Reporting and Output
  report_to: ["tensorboard"]
  
  # Optional Debug Parameters
  debug_dataset_filtering: false
  checkpoint_debug: false
  
  # Optional Advanced Parameters
  metric_for_best_model: "eval_loss"

models:
  LLaMA2-7B:
    model_name_or_path: "meta-llama/Llama-2-7b-hf"
    tokenizer_name: "meta-llama/Llama-2-7b-hf"
    train_type: "causal_lm"
    load_in_4bit: false  # Disable quantization for LoRA compatibility
    defaults:
      rank: 16
      max_len: 2048
      batch_size: 4
      kl_loss_beta: 0.01  # Lower KL strength for large model
      learning_rate: 0.0001  # Override learning rate for large model
      # Memory optimization for 40GB A100
      use_cache: false  # Disable KV caching
      gradient_checkpointing: true  # Enable gradient checkpointing

datasets:
  BayesianPEFT:
    dataset_type: "bayesian_peft"
    repo_url: "https://github.com/Wang-ML-Lab/bayesian-peft"
    dataset_path: "dataset"
    name: "bayesian_peft"
    max_len: 2048
    num_labels: 0
    cache_dir: "/content/drive/MyDrive/ARD_LoRA_Data_Cache"

    # Commonsense / Reasoning
    piqa:
      dataset_name: "piqa"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    hellaswag:
      dataset_name: "hellaswag"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 4
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    winogrande:
      dataset_name: "winogrande"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    arc_easy:
      dataset_name: "arc_easy"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 4
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    arc_challenge:
      dataset_name: "arc_challenge"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 4
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Reading comprehension
    boolq:
      dataset_name: "boolq"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Out-of-distribution / NLI-style evaluation
    anli:
      dataset_name: "anli"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 3
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    rte:
      dataset_name: "rte"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    cb:
      dataset_name: "cb"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 3
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    copa:
      dataset_name: "copa"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Sentiment classification
    sst2:
      dataset_name: "sst2"
      dataset_class: "S2ClassDataset"
      max_len: 512
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Instruction-following evaluation (not training)
    alpacaeval:
      dataset_name: "alpacaeval"
      dataset_class: "EvalOnlyDataset"
      max_len: 2048
      num_labels: 0
      notes: "Evaluation-only benchmark for instruction-following, not used for training"

    # Dataset configurations for different datasets (existing)
    alpaca:
      dataset_name: "alpaca"  # Specific dataset from Bayesian-PEFT suite
      dataset_class: "S2SDataset"  # Use their S2S dataset class
      max_len: 2048
      num_labels: 0
    dolly:
      dataset_name: "dolly"
      dataset_class: "S2SDataset"
      max_len: 2048
      num_labels: 0
    glue_cola:
      dataset_name: "glue_cola"
      dataset_class: "S2ClassDataset"  # Use their classification dataset class
      max_len: 512
      num_labels: 2
    
  # Additional datasets that could be used with ARD-LoRA
  Alpaca:
    dataset_type: "alpaca"
    max_len: 2048
    num_labels: 0
    
  Dolly:
    dataset_type: "dolly"  
    max_len: 2048
    num_labels: 0
