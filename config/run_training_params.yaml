# LLaMA-2-7B causal LM experiments with ARD-LoRA and Bayesian-PEFT datasets
defaults:
  runId: 1
  # Model configuration
  model_name: "LLaMA2-7B"
  model_name_or_path: "meta-llama/Llama-2-7b-hf"
  tokenizer_name: null  # Use model tokenizer
  train_type: "causal_lm"  # Specify training type for validation
  attn_implementation: "sdpa"  # Use SDPA for A100 optimization
  dataset_name: "BayesianPEFT"
  dataset_name_specific: "sst2"  # Which specific dataset from Bayesian-PEFT
  
  # Cache configuration
  cache_root: "/content/drive/MyDrive/ARD_LoRA_Data_Cache"  # Google Drive cache
  google_drive_cache: "/content/drive/MyDrive/ARD_LoRA_Data_Cache"  # Google Drive path
  use_google_drive: true  # Enable Google Drive caching
  
  # Training hyperparameters (adapted from Section 4.1 of BLoB / best-effort)
  learning_rate: 0.00001  # Reduced from 0.0001 to 0.00001 for stability
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.0
  train_epochs: 3
  batch_size: 16  # Increased from 4 to 16 for better A100 utilization
  gradient_accumulation_steps: 4  # Reduced from 16 to 4 (maintains effective batch size of 64)
  
  # Precision Configuration
  fp16: false
  bf16: true  # Preferred on A100 GPUs for better performance
  load_in_4bit: false  # Disable quantization - LoRA parameters need to be trainable
  
  # Training Strategy
  save_strategy: "epoch"
  eval_strategy: "epoch"
  save_steps: 500
  eval_steps: null
  load_best_model_at_end: true
  remove_unused_columns: false
  
  # Optimization
  max_grad_norm: 0.1  # Much more aggressive clipping to prevent instability
  optim: "adamw_torch"
  gradient_checkpointing: true
  use_cache: false  # Disable KV caching during training to save memory
  
  # Logging and Monitoring
  logging_steps: 10  # Increased from 50 to 100 to reduce logging overhead
  dataloader_num_workers: 4  # Increased from 0 to 4 for better data loading
  dataloader_pin_memory: True

  # ARD-LoRA Configuration
  rank: 16
  scaling: 0.25
  max_lora_rank_threshold: 64
  
  # ProbLoRA Model Parameters
  num_tokens: 1024  # Updated to match max_len
  ard_prior_samples: 25  # Reduced from 100 to 25 for faster training
  
  # Numerical Stability Parameters - ULTRA CONSERVATIVE
  logvar_clamp_min: -5.0  # Very tight bounds to prevent overflow
  logvar_clamp_max: 5.0   # Very tight bounds to prevent overflow
  beta_logvar_clamp_min: -3.0  # Even tighter for beta sampling
  beta_logvar_clamp_max: 3.0   # Even tighter for beta sampling
  sample_clamp_min: -3.0  # Very tight sample bounds
  sample_clamp_max: 3.0   # Very tight sample bounds
  
  # ARD Loss Configuration
  kl_loss_beta: 0.01  # Further reduced from 0.001 to 0.0001 to prevent loss explosion
  
  # ARD Relevance Thresholds
  ard_high_relevance_threshold: 0.1
  ard_medium_relevance_threshold: 0.01
  
  # Uncertainty Evaluation
  uncertainty_eval_samples: 250  # Reduced from 1000 to 250 for faster evaluation
  uncertainty_n_bins: 15
  
  # Dataset Configuration - Global Settings
  max_len: 1024  # Reduced from 2048 to 1024 for stability and speed
  num_labels: 0
  validation_split_ratio: 0.1
  min_validation_samples: 2000
  max_ard_ratio: 0.8
  random_seed: 42
  max_validation_samples: 250  # Cap validation dataset size for memory efficiency
  
  # Callback Configuration - DISABLED FOR STABILITY
  enable_callbacks: false  # Temporarily disable ARD callbacks to isolate issues
  enable_plotting: false   # Disabled to reduce overhead
  enable_resampling: false # Disabled to reduce overhead
  plot_start_epoch: 2
  plot_interval: 2
  plot_batch_size: 16
  
  # Memory Optimization
  gpu_memory_fraction: 0.9
  gradient_checkpointing_kwargs:
    use_reentrant: false
  
  # Reporting and Output
  report_to: ["tensorboard"]
  
  # Optional Debug Parameters
  debug_dataset_filtering: false
  checkpoint_debug: false
  
  # Optional Advanced Parameters
  metric_for_best_model: "eval_loss"

models:
  LLaMA2-7B:
    model_name_or_path: "meta-llama/Llama-2-7b-hf"
    tokenizer_name: "meta-llama/Llama-2-7b-hf"
    train_type: "causal_lm"
    load_in_4bit: false  # Disable quantization for LoRA compatibility
    defaults:
      rank: 16
      max_len: 2048  # Uses global max_len setting
      batch_size: 4
      kl_loss_beta: 0.01  # Lower KL strength for large model
      learning_rate: 0.0001  # Override learning rate for large model
      # Memory optimization for 40GB A100
      use_cache: false  # Disable KV caching
      gradient_checkpointing: true  # Enable gradient checkpointing

datasets:
  BayesianPEFT:
    dataset_type: "bayesian_peft"
    repo_url: "https://github.com/Wang-ML-Lab/bayesian-peft"
    dataset_path: "dataset"
    name: "bayesian_peft"
    max_len: 2048  # Uses global max_len setting
    num_labels: 0
    cache_dir: "/content/drive/MyDrive/ARD_LoRA_Data_Cache"

    # Commonsense / Reasoning
    piqa:
      dataset_name: "piqa"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    hellaswag:
      dataset_name: "hellaswag"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 4
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    winogrande:
      dataset_name: "winogrande"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    arc_easy:
      dataset_name: "arc_easy"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 4
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    arc_challenge:
      dataset_name: "arc_challenge"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 4
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Reading comprehension
    boolq:
      dataset_name: "boolq"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Out-of-distribution / NLI-style evaluation
    anli:
      dataset_name: "anli"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 3
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    rte:
      dataset_name: "rte"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    cb:
      dataset_name: "cb"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 3
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    copa:
      dataset_name: "copa"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Sentiment classification
    sst2:
      dataset_name: "sst2"
      dataset_class: "S2ClassDataset"
      max_len: 2048  # Updated to global max_len
      num_labels: 2
      validation_split_ratio: 0.1
      min_validation_samples: 2000
      max_ard_ratio: 0.8
      random_seed: 42

    # Instruction-following evaluation (not training)
    alpacaeval:
      dataset_name: "alpacaeval"
      dataset_class: "EvalOnlyDataset"
      max_len: 2048  # Uses global max_len setting
      num_labels: 0
      notes: "Evaluation-only benchmark for instruction-following, not used for training"

    # Dataset configurations for different datasets (existing)
    alpaca:
      dataset_name: "alpaca"  # Specific dataset from Bayesian-PEFT suite
      dataset_class: "S2SDataset"  # Use their S2S dataset class
      max_len: 2048  # Uses global max_len setting
      num_labels: 0
    dolly:
      dataset_name: "dolly"
      dataset_class: "S2SDataset"
      max_len: 2048  # Uses global max_len setting
      num_labels: 0
    glue_cola:
      dataset_name: "glue_cola"
      dataset_class: "S2ClassDataset"  # Use their classification dataset class
      max_len: 2048  # Updated to global max_len
      num_labels: 2
    
  # Additional datasets that could be used with ARD-LoRA
  Alpaca:
    dataset_type: "alpaca"
    max_len: 2048  # Uses global max_len setting
    num_labels: 0
    
  Dolly:
    dataset_type: "dolly"  
    max_len: 2048  # Uses global max_len setting
    num_labels: 0
