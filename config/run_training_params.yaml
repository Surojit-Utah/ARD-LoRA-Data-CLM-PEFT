# LLaMA-2-7B causal LM experimedatasets:
  BayesianPEFT:
    dataset_name: "alpaca"  # Specific dataset from Bayesian-PEFT suite
    dataset_class: "S2SDataset"  # Use their S2S dataset class
    repo_url: "https://github.com/Wang-ML-Lab/bayesian-peft"
    cache_root: "./data_cache"  # Local cache directory (can be Google Drive mount)
    max_len: 2048
    num_labels: 0
    
  # Alternative configurations for other datasets
  BayesianPEFT_Dolly:
    dataset_name: "dolly"
    dataset_class: "S2SDataset"
    repo_url: "https://github.com/Wang-ML-Lab/bayesian-peft"
    cache_root: "./data_cache"
    max_len: 2048
    num_labels: 0
    
  BayesianPEFT_Classification:
    dataset_name: "glue_cola"
    dataset_class: "S2ClassDataset"  # Use their classification dataset class
    repo_url: "https://github.com/Wang-ML-Lab/bayesian-peft"
    cache_root: "./data_cache"
    max_len: 512
    num_labels: 2PEFT datasets)
defaults:
  runId: 1
  # Cache configuration
  cache_root: "./data_cache"  # Default local cache
  google_drive_cache: "/content/drive/MyDrive/ARD_LoRA_Data_Cache"  # Google Drive path
  use_google_drive: true  # Enable Google Drive caching
  # Hyperparameters (adapted from Section 4.1 of BLoB / best-effort)
  learning_rate: 1e-5
  lr_scheduler_type: "linear"
  warmup_ratio: 0.03
  weight_decay: 0.0
  train_epochs: 1
  rank: 64
  max_len: 2048
  ard_prior_samples: 100
  batch_size: 4
  gradient_accumulation_steps: 32
  fp16: false
  bf16: true  # Preferred on A100 GPUs for better performance
  load_in_4bit: true
  prior_var: 1.0
  kl_loss_beta: 0.01  # KL divergence loss strength for ARD-LoRA
  max_validation_samples: 5000  # Cap validation dataset size for memory efficiency

models:
  LLaMA2-7B:
    model_name_or_path: "meta-llama/Llama-2-7b-hf"
    tokenizer_name: "meta-llama/Llama-2-7b-hf"
    train_type: "causal_lm"
    load_in_4bit: true
    defaults:
      rank: 64
      max_len: 2048
      batch_size: 8
      kl_loss_beta: 0.005  # Lower KL strength for large model

datasets:
  BayesianPEFT:
    dataset_type: "bayesian_peft"  # Use our new dataset strategy
    repo_url: "https://github.com/Wang-ML-Lab/bayesian-peft"
    dataset_path: "dataset"
    name: "bayesian_peft"
    max_len: 2048
    num_labels: 0
    cache_dir: "./data_cache"  # Local caching directory
    
  # Additional datasets that could be used with ARD-LoRA
  Alpaca:
    dataset_type: "alpaca"
    max_len: 2048
    num_labels: 0
    
  Dolly:
    dataset_type: "dolly"  
    max_len: 2048
    num_labels: 0
